{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj9Sf-_rdCWX",
        "outputId": "77302cc5-b987-4a8b-de75-e8417bd1c405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4 requests sentence-transformers faiss-cpu openai transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "0wVkddDHd5On"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code:\n"
      ],
      "metadata": {
        "id": "K1XsJ8CFgIFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import openai\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import re\n",
        "\n",
        "# Step 1: Crawl and Scrape COntent of the website\n",
        "def scrape_website(url):\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract text from all paragraphs, headers, so on excetra....\n",
        "    paragraphs = soup.find_all(['p', 'h1', 'h2', 'h3', 'ul', 'li'])\n",
        "    text = \" \".join([para.get_text() for para in paragraphs])\n",
        "\n",
        "    # Clean up any unnecessary spaces or special characters,using REgular EXPRESSIONS\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Step 2: Chunking and Embedding Content\n",
        "def chunk_and_embed(text, model, max_chunk_size=512):\n",
        "\n",
        "    chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "    embeddings = model.encode(chunks)\n",
        "    return chunks, embeddings\n",
        "\n",
        "# Step 3: Store Embeddings in a Vector Database (FAISS)\n",
        "def store_embeddings(embeddings):\n",
        "\n",
        "    db = faiss.IndexFlatL2(len(embeddings[0]))\n",
        "    db.add(embeddings)\n",
        "    return db\n",
        "\n",
        "# Step 4: Query Handling - Search Similarity in the Vector Database\n",
        "def query_vector_db(query, model, db, chunks, k=5):\n",
        "\n",
        "    query_vector = model.encode([query])\n",
        "    distances, indices = db.search(query_vector, k)\n",
        "    results = [chunks[i] for i in indices[0]]\n",
        "    return results\n",
        "\n",
        "# Step 5: Response Generation Using GPT-2 (or similar LLM)\n",
        "def generate_response(chunks, query, retries=3, backoff=5):\n",
        "\n",
        "    # Prepare the prompt\n",
        "    prompt = f\"Based on the following content, answer the question: {query}\\n\\n\"\n",
        "    for chunk in chunks:\n",
        "        prompt += f\"- {chunk}\\n\"\n",
        "\n",
        "    # Load the GPT-2 model and tokenizer\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_name = \"gpt2\"\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "# or\n",
        "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    # Set pad_token_id to eos_token_id for GPT-2\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "    # Tokenize the prompt and handle the attention mask\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True).to(device)\n",
        "\n",
        "    # Retry logic for generating response\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Generate response\n",
        "            outputs = model.generate(\n",
        "                inputs['input_ids'],\n",
        "                attention_mask=inputs['attention_mask'],  # Attention mask is set\n",
        "                max_new_tokens=200,  # Set max_new_tokens instead of max_length to avoid conflicts\n",
        "                do_sample=True,      # Ensure sampling for diversity\n",
        "                no_repeat_ngram_size=2,\n",
        "                top_p=0.95,\n",
        "                top_k=50,\n",
        "                temperature=0.7,     # Adjust temperature for creativity\n",
        "            )\n",
        "\n",
        "            # Decode the generated response\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            time.sleep(backoff)  # Retry after a backoff period\n",
        "            continue\n",
        "\n",
        "    return \"Unable to generate response due to an error.\"\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.uchicago.edu/\"  # URL of the website to scrape\n",
        "\n",
        "    # Step 1: Scrape the website content\n",
        "    scraped_content = scrape_website(url)\n",
        "\n",
        "    # Step 2: Initialize the embedding model and process the scraped text\n",
        "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    chunks, embeddings = chunk_and_embed(scraped_content, embedding_model)\n",
        "\n",
        "    # Step 3: Store embeddings in the FAISS vector database\n",
        "    faiss_db = store_embeddings(embeddings)\n",
        "\n",
        "    # Step 4: Query the vector database\n",
        "    query = \"What is the University of Chicago known for?\"\n",
        "    relevant_chunks = query_vector_db(query, embedding_model, faiss_db, chunks)\n",
        "\n",
        "    # Step 5: Generate a response using GPT-2 (or other LLM)\n",
        "    response = generate_response(relevant_chunks, query)\n",
        "    print(\"Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KekNOMEafZ8T",
        "outputId": "d5e8fc5f-9ddc-44fc-c140-c3ee59bbe4c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Based on the following content, answer the question: What is the University of Chicago known for?\n",
            "\n",
            "- 20241217T024330Z-1577f6f44dbgtczfhC1ATL819w0000000h100000000092q4\n",
            "- 20241217T024330Z-1577f6f44dbgtczfhC1ATL819w0000000h100000000092q4\n",
            "- 20241217T024330Z-1577f6f44dbgtczfhC1ATL819w0000000h100000000092q4\n",
            "- 20241217T024330Z-1577f6f44dbgtczfhC1ATL819w0000000h100000000092q4\n",
            "- 20241217T024330Z-1577f6f44dbgtczfhC1ATL819w0000000h100000000092q4\n",
            " and we will find that the university is known as the \"National University.\"\n",
            ".\n",
            " - 20241301Z158850e2f1b8c4b2a838a0a4f8f9c3e1f7e3c1c2c64b1e7f2d65d3a9b5f5b6a7\n",
            "The University is also known to be the home of the most famous and influential people in the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kz_hGpJMf8fO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}